{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate tuned performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import altair as alt \n",
    "alt.data_transformers.disable_max_rows() \n",
    "from ODD.analysis.validation_set import (\n",
    "    get_average_performance_per_dataset,\n",
    "    generate_validation_sets_statistical, \n",
    "    generate_validation_sets_absolute, \n",
    "    generate_validation_sets_absolute_subset, \n",
    "    generate_validation_sets_absolute_subset_multiple_runs, \n",
    "    generate_validation_sets_statistical_subset_multiple_runs,\n",
    "    get_data_df\n",
    ")\n",
    "from ODD.analysis.hyperparameters import select_best_validation_performance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.random import default_rng\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from dask.distributed import Client\n",
    "from tqdm import tqdm\n",
    "from numpy.random import default_rng\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_version_to_use = defaultdict(lambda: 1)\n",
    "grid_version_to_use['HBOS'] = 2\n",
    "grid_version_to_use['CBLOF'] = 2\n",
    "grid_version_to_use['IForest'] = 2\n",
    "grid_version_to_use['OCSVM'] = 3\n",
    "algorithms = ['CBLOF', 'HBOS', 'IForest', 'KNN' , 'LOF', 'OCSVM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = Path()/'results'\n",
    "config_path = Path()/'config'\n",
    "processed_path = Path()/'processed_results_v5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERWRITE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the auc thresholds to use for statistical validation set size\n",
    "For this we use the average default performance per datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_thresholds = (\n",
    "    get_average_performance_per_dataset(processed_path / 'default_performance')\n",
    "    .auc.to_frame('auc_threshold')\n",
    ")\n",
    "auc_thresholds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets to use \n",
    "If original contamination is <20%, use the original dataset otherwise use one of the subsamples versions at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_to_use = pd.read_pickle(Path()/'used_datasets'/'used_stat_datasets.pkl').drop(columns = ['index'])\n",
    "datasets_to_use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the labels of the datasets we will use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all datasets with label information\n",
    "full_data_df = get_data_df(result_path/'grid_HBOS_v2.pkl')\n",
    "# only datasets to use\n",
    "data_df = full_data_df.groupby(['dataset_id', 'anomaly_fraction']).filter(lambda x: x.name in datasets_to_use.index)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latex code for dataset information table in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = (\n",
    "    data_df.sort_index()\n",
    "    .drop(columns = 'dataset_version')\n",
    "    .assign(\n",
    "        nb_instances = lambda x: x.labels.apply(len), \n",
    "        nb_anomalies = lambda x: x.labels.apply(np.sum), \n",
    "        contamination = lambda x: (x.nb_anomalies/x.nb_instances*100).apply(lambda y: f\"{y:.1f}%\")\n",
    "    )\n",
    "    .droplevel(level = 1)\n",
    "    .drop(columns = ['labels', 'nb_anomalies'])\n",
    ")\n",
    "print(temp_df.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tuned_performance_multiple_runs(validation_dfs, experiment_name, setting_name, record, dask_client = None, overwrite = OVERWRITE):\n",
    "    \"\"\"\n",
    "        Given a list of dataframes with validation set information, calculate the tuned performance and write to disk\n",
    "    \"\"\"\n",
    "    res_path = processed_path /experiment_name / setting_name\n",
    "    \n",
    "    for run_idx, validation_df in enumerate(validation_dfs): \n",
    "        # save the validation df for later reference \n",
    "        run_path = res_path/f\"run{run_idx}\"\n",
    "        run_path.mkdir(parents = True, exist_ok = True)\n",
    "        \n",
    "        # for reference store the validation set you used\n",
    "        validation_df.to_csv(run_path / 'validation_set.csv')\n",
    "        validation_df.to_pickle(run_path / 'validation_set.pkl')\n",
    "        \n",
    "        # for each algorithm calculate best performance based on validation set\n",
    "        best_performance = None\n",
    "        iterator = tqdm(algorithms)\n",
    "        for algo in iterator: \n",
    "            iterator.set_description(algo)\n",
    "            grid_version = grid_version_to_use[algo]\n",
    "            if not overwrite and (run_path/f\"{algo}.pkl\").exists(): \n",
    "                continue\n",
    "            # read the grid \n",
    "            iterator.set_postfix({'status': 'reading grid'})\n",
    "            result_df = pd.read_pickle(result_path/ f'grid_{algo}_v{grid_version}.pkl')\n",
    "\n",
    "            # select the best performance based on the validation set \n",
    "            iterator.set_postfix({'status': 'selecting best validation set'})\n",
    "            best_performance = select_best_validation_performance(result_df, validation_df, dask_client = dask_client)\n",
    "\n",
    "            # assign the record information \n",
    "            iterator.set_postfix({'status': 'adding additional information'})\n",
    "            best_performance = best_performance.assign(run_idx = run_idx, **record)\n",
    "\n",
    "            # save the result\n",
    "            iterator.set_postfix({'status': 'saving results'})\n",
    "            best_performance.to_csv(run_path/f\"{algo}.csv\")\n",
    "            best_performance.to_pickle(run_path/f\"{algo}.pkl\")\n",
    "    \n",
    "        print('saving dataset information')\n",
    "        if best_performance is not None: \n",
    "            dataset_info = best_performance[['dataset_id', 'anomaly_fraction', 'dataset_version']].drop_duplicates().reset_index(drop = True)\n",
    "            dataset_info.to_csv(processed_path/experiment_name/setting_name/f'run{run_idx}'/'dataset_info.csv', index = False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical size multiple runs\n",
    "Does 10 runs, for each validation set size with p = 0.01, 0.05, 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistical_subset_validation_set_experiment_multiple_runs(data_df,auc_threshold, significance_ps, experiment_name, n_runs = 5, reasonable_threshold = 0.5, dask_client = None, seed =None): \n",
    "    print('generating validation sets')\n",
    "    all_reference_dfs, all_validation_dfs = generate_validation_sets_statistical_subset_multiple_runs(data_df, auc_threshold, significance_ps, n_runs, reasonable_threshold = reasonable_threshold, seed = seed)\n",
    "                                          \n",
    "    print('saving reference dfs')\n",
    "    (processed_path/experiment_name).mkdir(parents = True, exist_ok = True)\n",
    "    for run_idx, reference_df in enumerate(all_reference_dfs):\n",
    "        reference_df.to_csv(processed_path/experiment_name/f'reference_validation_run{run_idx}.csv')\n",
    "        reference_df.to_pickle(processed_path/experiment_name/f'reference_validation_run{run_idx}.pkl')\n",
    "    \n",
    "    for size, validation_dfs in all_validation_dfs.items(): \n",
    "        print(f'calculating tuned performance for size = {size}')\n",
    "        record = dict(size = size)\n",
    "        calculate_tuned_performance_multiple_runs(validation_dfs, experiment_name, f\"size={size}\", record, dask_client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Client(n_workers = 40, local_directory = '/cw/dtailocal/jonass/') as client: \n",
    "    statistical_subset_validation_set_experiment_multiple_runs(data_df, auc_thresholds, [0.01, 0.05, 0.10], experiment_name = 'statistical_validation_set_multiple_10runs', n_runs = 10, dask_client = client,  seed = 12341324)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Absolute size with multiple runs\n",
    "The bigger size is always a superset of the smaller set, do 10 runs for size = 50, 100, 150,200,250 and use 25% of the labeled data at most.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absolute_subset_validation_set_experiment_multiple_runs_upper_limit(data_df,absolute_sizes, experiment_name, n_runs = 5, upper_limit = 0.25, reasonable_threshold = 0.5, dask_client = None, seed =None): \n",
    "    print('generating validation sets')\n",
    "    all_reference_dfs, all_validation_dfs = generate_validation_sets_absolute_subset_multiple_runs(data_df, absolute_sizes, n_runs,upper_limit = upper_limit, reasonable_threshold = reasonable_threshold, seed = seed)\n",
    "                                          \n",
    "    print('saving reference dfs')\n",
    "    (processed_path/experiment_name).mkdir(parents = True, exist_ok = True)\n",
    "    for run_idx, reference_df in enumerate(all_reference_dfs):\n",
    "        reference_df.to_csv(processed_path/experiment_name/f'reference_validation_run{run_idx}.csv')\n",
    "        reference_df.to_pickle(processed_path/experiment_name/f'reference_validation_run{run_idx}.pkl')\n",
    "    \n",
    "    for size, validation_dfs in all_validation_dfs.items(): \n",
    "        print(f'calculating tuned performance for size = {size}')\n",
    "        record = dict(size = size)\n",
    "        calculate_tuned_performance_multiple_runs(validation_dfs, experiment_name, f\"size={size}\", record, dask_client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Client(n_workers = 40, local_directory = '/cw/dtailocal/jonass/') as client: \n",
    "    absolute_subset_validation_set_experiment_multiple_runs_upper_limit(data_df,  [250,200, 150, 100, 50] , 'absolute_validation_set_multiple_10runs_max_25', 10, dask_client = client, seed = 1231234234)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
