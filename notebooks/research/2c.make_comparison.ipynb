{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make comparison between different hyperparameter selection strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the result processing later on easier, we already preprocess the different strategies a bit and put them all in the same dataframe for easy comparison. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from ODD.analysis.hyperparameters import select_peak_performance, select_best_average_performance, calculate_best_average_performance, calculate_validation_set_performances\n",
    "from ODD.analysis.result_processing import average_performance_per_method, average_aligned_ranks_with_versions, average_ranks_with_versions, average_ranks_with_versions_and_nemenyi\n",
    "from ODD.analysis.result_analysis_charts import *\n",
    "from ODD.analysis.dataset_selection import get_datasets_to_use\n",
    "from ODD.analysis.validation_set import  get_data_df\n",
    "from tqdm import tqdm\n",
    "import altair as alt \n",
    "alt.data_transformers.disable_max_rows()\n",
    "# alt.renderers.enable('png')\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 1 \n",
    "algorithms = ['CBLOF', 'HBOS', 'IForest', 'KNN' , 'LOF', 'OCSVM']\n",
    "REFERENCES = ['default_performance', 'peak_performance', 'best_average_performance']\n",
    "grid_versions_to_use = defaultdict(lambda: 1)\n",
    "grid_versions_to_use['HBOS'] = 2\n",
    "grid_versions_to_use['IForest'] = 2\n",
    "grid_versions_to_use['CBLOF'] = 2\n",
    "grid_versions_to_use['OCSVM'] = 3\n",
    "result_path = Path()/'results'\n",
    "processed_path = Path()/'processed_results_v5'\n",
    "comparison_path = Path()/'comparisons'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_to_use = pd.read_pickle(Path()/'used_datasets'/'used_stat_datasets.pkl')\n",
    "datasets_to_use;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS_TO_KEEP = ['significance_p', 'size', 'anom_multiplier', 'run_idx']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_algos_from_dir(path, datasets_to_use): \n",
    "    result_dfs = [pd.read_pickle(path/f\"{algo}.pkl\") for algo in algorithms]\n",
    "    result_df = pd.concat(result_dfs, axis = 0, keys = algorithms, names = ['algorithm_name'])\n",
    "    result_df = result_df.groupby(['dataset_id', 'anomaly_fraction']).filter(lambda x: x.name in datasets_to_use.index).reset_index()\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_performance_df_multiple_runs(experiment_name,datasets_to_use, reference_validation_dfs): \n",
    "    path = processed_path/experiment_name\n",
    " \n",
    "    # look at all experiments \n",
    "    result_directories = [directory for directory in path.iterdir() if (directory.is_dir() and not directory.name.startswith('.'))]\n",
    "    \n",
    "    nb_runs = len(reference_validation_dfs)\n",
    "    \n",
    "    dfs = []\n",
    "    columns_to_keep = None \n",
    "    parameter_columns = None\n",
    "    # for each setting calculate the average performance on the test set \n",
    "    for setting_path in result_directories: \n",
    "        for run_idx, reference_validation_df in enumerate(reference_validation_dfs): \n",
    "            run_path = setting_path/f'run{run_idx}'\n",
    "            result_df = read_all_algos_from_dir(run_path, datasets_to_use)\n",
    "            if columns_to_keep is None: \n",
    "                columns_to_keep  = [column for column in COLUMNS_TO_KEEP if column in result_df.columns]\n",
    "                parameter_columns = list(columns_to_keep)\n",
    "                parameter_columns.remove('run_idx')\n",
    "            result_df = (\n",
    "                result_df \n",
    "                # drop the test_auc and test_ap \n",
    "                .drop(columns = ['test_auc', 'test_ap', 'validation_auc'])\n",
    "                # recalculate test_auc and test_ap using standard validation set\n",
    "                .pipe(lambda x: calculate_validation_set_performances(x, reference_validation_df))\n",
    "            )\n",
    "            dfs.append(result_df)\n",
    "    \n",
    "    validation_performance_df = (\n",
    "        # add all the dfs together\n",
    "        pd.concat(dfs, axis = 0).reset_index()\n",
    "        # take average and std over runs \n",
    "        .groupby(['algorithm_name', 'dataset_id'] + parameter_columns)[['test_auc', 'test_ap']].mean()\n",
    "        .rename(columns = {'test_auc': 'auc', 'test_ap':'ap'})\n",
    "        .assign(\n",
    "            reference = 'tuned'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return validation_performance_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reference_performance_df_multiple_runs(references, datasets_to_use, reference_validation_dfs): \n",
    "    dfs = []\n",
    "    nb_runs = len(reference_validation_dfs)\n",
    "    for reference in references: \n",
    "        path = processed_path/reference\n",
    "        \n",
    "        result_df = read_all_algos_from_dir(path, datasets_to_use)\n",
    "        for run_idx, reference_validation_df in enumerate(reference_validation_dfs): \n",
    "            performance_df = (\n",
    "                result_df\n",
    "                # calculate test_ap and test_auc\n",
    "                .pipe(lambda x: calculate_validation_set_performances(x, reference_validation_df))\n",
    "                #average performance over the datasets\n",
    "                .assign(reference = reference)\n",
    "            )\n",
    "            dfs.append(performance_df.reset_index())\n",
    "        \n",
    "\n",
    "    result_df = (\n",
    "        pd.concat(dfs, axis = 0)\n",
    "        # average performance over runs \n",
    "        .groupby(['reference', 'algorithm_name', 'dataset_id'])[['test_auc', 'test_ap']].mean().reset_index()\n",
    "        .rename(columns = {'test_auc':'auc', 'test_ap':'ap'})\n",
    "        .assign(\n",
    "            reference = lambda x: x.reference.replace({'default_performance': 'out-of-the-box', 'peak_performance':'peak', 'best_average_performance':'best-default'})\n",
    "        )\n",
    "    )\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_and_reference_dfs_multiple_runs(validation_experiment, references, datasets_to_use): \n",
    "    # get the datasets to use \n",
    "    print(f\"using {len(datasets_to_use)} datasets in the comparison\")\n",
    "    \n",
    "    reference_df_paths = sorted((processed_path/validation_experiment).glob('reference_validation_run*.pkl'), key = lambda x: x.name)\n",
    "    \n",
    "    # get the reference_validation_df \n",
    "    reference_validation_dfs = [pd.read_pickle(reference_df_path).rename(columns = {'reference_validation_set':'validation_indices'}) for reference_df_path in reference_df_paths]\n",
    "    \n",
    "    # calculate the best average performance using the correct datasets\n",
    "    if 'best_average_performance' in references: \n",
    "        print('calculating best average performance')\n",
    "        calculate_best_average_performance(Path()/'results', algorithms, grid_versions_to_use, datasets_to_use, processed_path/'best_average_performance')\n",
    "\n",
    "    # get the validation settings dataframe\n",
    "    full_validation_performance_df = get_validation_performance_df_multiple_runs(\n",
    "        validation_experiment,\n",
    "        datasets_to_use,\n",
    "        reference_validation_dfs\n",
    "    )\n",
    "    # get the references settings dataframe\n",
    "    full_reference_df = get_reference_performance_df_multiple_runs(\n",
    "        references, \n",
    "        datasets_to_use, \n",
    "        reference_validation_dfs\n",
    "    )\n",
    "    \n",
    "    return full_validation_performance_df, full_reference_df\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_comparison(experiment_name, comparison_name): \n",
    "    validation_df, reference_df = get_validation_and_reference_dfs_multiple_runs(experiment_name, REFERENCES, datasets_to_use)\n",
    "    path = comparison_path / comparison_name\n",
    "    path.mkdir(parents = True, exist_ok = True)\n",
    "    combined = pd.concat([validation_df, reference_df], axis = 0, ignore_index =True)\n",
    "    combined.to_csv(path/'comparison.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the actual comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_comparison('statistical_validation_set_multiple_10runs', 'statistical_validation_set_size')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
